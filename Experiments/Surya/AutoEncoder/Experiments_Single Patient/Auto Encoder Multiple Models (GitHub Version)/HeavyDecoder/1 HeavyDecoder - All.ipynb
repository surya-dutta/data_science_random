{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0ea8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a notebook to summarize all different models.\n",
    "# THis notebook was not run\n",
    "\n",
    "\n",
    "## Heavy Decoder\n",
    "# encoder_dense_layers_trial = [[10, 8], [12, 10], [14, 12], [16, 14], [18, 16], [20, 18],[22, 20]]\n",
    "# decoder_dense_layers_trial = [[6, 8, 10, 12], [8, 10, 12, 14], [10, 12, 14, 16], [12, 14, 16, 18], [14, 16, 18, 20], \n",
    "#                               [16, 18, 20, 22], [18, 20, 22, 24]]\n",
    "# bottle_neck_trial = [8, 10, 12, 14, 16, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a7cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable Warning\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9338f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE1 = 'SB-001_normalized_AM.csv'\n",
    "FILE2 = 'SB-001_normalized_PM.csv'\n",
    "NO_OF_COLUMNS = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59a794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tf2onnx\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import cv2\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings('ignore', message=r'.*Consider either turning off auto-sharding.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beb3d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d23a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_autoencoder_reports(encoder_dense_layers, bottle_neck, decoder_dense_layers):\n",
    "\n",
    "    print(encoder_dense_layers, bottle_neck, decoder_dense_layers)\n",
    "    # Load Data\n",
    "    df_am = pd.read_csv(FILE1)\n",
    "    df_pm = pd.read_csv(FILE2)\n",
    "    combined_df = pd.concat([df_am, df_pm])\n",
    "\n",
    "    columns_needed = ['y_am_pef', 'tempin', 'humidin', 'pm25in', 'co2in', 'tempdiffin', 'humidiffin', 'pm25diffin',\n",
    "                      'pm10', 'pm25', 'o3', 'no2', 'co', 'so2', 'temp', 'windsd', 'humid', 'varp', 'dewpt', 'airp',\n",
    "                      'seap', 'solrhr', 'solramnt', 'grdt', 'class']\n",
    "    combined_df = combined_df.filter(columns_needed)\n",
    "    df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Drop NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Minotity Split\n",
    "    class_counts = df['class'].value_counts()\n",
    "    minority_class = df[df['class'] == 0]\n",
    "    majority_class = df[df['class'] == 1]\n",
    "\n",
    "    # Removing class column from minority before augmentation\n",
    "    X = minority_class.drop('class', axis=1)\n",
    "    y = minority_class['class']\n",
    "\n",
    "    # Saving minority as X_train\n",
    "    X_train = X\n",
    "\n",
    "    # Define the Input shape\n",
    "    INPUT_SHAPE = X_train.shape[1]\n",
    "    FILE_NAME = f\"L{INPUT_SHAPE}_E{'_'.join(map(str, encoder_dense_layers))}_B{bottle_neck}_D{'_'.join(map(str, decoder_dense_layers))}\"\n",
    "    print(\"\\n\" + FILE_NAME + \"\\n\")\n",
    "    \n",
    "    def build_autoencoder(input_shape, **kwargs):\n",
    "\n",
    "        # Autoencoder model construction cod\n",
    "\n",
    "      \"\"\"\n",
    "      Build an autoencoder model with the given configuration.\n",
    "\n",
    "      Args:\n",
    "          input_shape (int): The shape of the input data.\n",
    "          **kwargs: Additional keyword arguments for configuring the autoencoder.\n",
    "\n",
    "      Keyword Args:\n",
    "          encoder_dense_layers (list): List of integers specifying the number of units for each dense layer in the encoder.\n",
    "                                      Default: []\n",
    "          bottle_neck (int): The number of units in the bottleneck layer. Default: input_shape // 2\n",
    "          decoder_dense_layers (list): List of integers specifying the number of units for each dense layer in the decoder.\n",
    "                                      Default: []\n",
    "          decoder_activation (str): The activation function for the decoder output layer. Default: 'sigmoid'\n",
    "\n",
    "      Returns:\n",
    "          tuple: A tuple containing the autoencoder, encoder, and decoder models.\n",
    "\n",
    "      Example:\n",
    "          INPUT_SHAPE = 27\n",
    "          encoder_dense_layers = [20, 18]  # Specify the number of units for each dense layer in the encoder\n",
    "          bottle_neck = 16\n",
    "          decoder_dense_layers = [18, 20]  # Specify the number of units for each dense layer in the decoder\n",
    "\n",
    "          autoencoder, encoder, decoder = build_autoencoder(INPUT_SHAPE, encoder_dense_layers=encoder_dense_layers,\n",
    "                                                            bottle_neck=bottle_neck, decoder_dense_layers=decoder_dense_layers)\n",
    "          encoder.summary()\n",
    "          decoder.summary()\n",
    "          autoencoder.summary()\n",
    "      \"\"\"\n",
    "\n",
    "      # Default parameter values\n",
    "      encoder_dense_layers = kwargs.get('encoder_dense_layers', [])\n",
    "      bottle_neck = kwargs.get('bottle_neck', input_shape // 2)\n",
    "      decoder_dense_layers = kwargs.get('decoder_dense_layers', [])\n",
    "      decoder_activation = kwargs.get('decoder_activation', 'sigmoid')\n",
    "\n",
    "      # Autoencoder Model\n",
    "      encoder_input = keras.Input(shape=(input_shape,), name=\"encoder\")\n",
    "      x = keras.layers.Flatten()(encoder_input)\n",
    "\n",
    "      # Encoder Dense Layers\n",
    "      for units in encoder_dense_layers:\n",
    "          x = keras.layers.Dense(units, activation=\"relu\")(x)\n",
    "\n",
    "      encoder_output = keras.layers.Dense(bottle_neck, activation=\"relu\")(x)\n",
    "      encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "\n",
    "      # Decoder Model\n",
    "      decoder_input = keras.Input(shape=(bottle_neck,), name=\"decoder\")\n",
    "      x = decoder_input\n",
    "\n",
    "      # Decoder Dense Layers\n",
    "      for units in decoder_dense_layers:\n",
    "          x = keras.layers.Dense(units, activation=\"relu\")(x)\n",
    "\n",
    "      decoder_output = keras.layers.Dense(input_shape, activation=decoder_activation)(x)\n",
    "      decoder = keras.Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "\n",
    "      # Autoencoder Model\n",
    "      autoencoder_input = keras.Input(shape=(input_shape,), name=\"input\")\n",
    "      encoded = encoder(autoencoder_input)\n",
    "      decoded = decoder(encoded)\n",
    "      autoencoder = keras.Model(autoencoder_input, decoded, name=\"autoencoder\")\n",
    "\n",
    "      return autoencoder, encoder, decoder\n",
    "\n",
    "    autoencoder, encoder, decoder = build_autoencoder(INPUT_SHAPE, encoder_dense_layers=encoder_dense_layers,\n",
    "                                                      bottle_neck=bottle_neck,\n",
    "                                                      decoder_dense_layers=decoder_dense_layers)\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    autoencoder.compile(opt, loss=\"mse\")\n",
    "\n",
    "    print(\"Training model:\", FILE_NAME)\n",
    "    history = autoencoder.fit(X_train, X_train, epochs=200, batch_size=16, validation_split=0.25, verbose=0)\n",
    "    print(\"Training Complete:\")\n",
    "    \n",
    "    # Extract the loss values\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Print the last epoch's loss values\n",
    "    last_epoch_loss = loss[-1]\n",
    "    last_epoch_val_loss = val_loss[-1]\n",
    "    print(\"Last epoch loss:\", last_epoch_loss)\n",
    "    print(\"Last epoch validation loss:\", last_epoch_val_loss)\n",
    "\n",
    "    # Saving history\n",
    "    with open(FILE_NAME + '_history.pickle', 'wb') as file:\n",
    "        pickle.dump(history.history, file)\n",
    "\n",
    "    # Visualize losses *(MSE)\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(FILE_NAME + 'Loss.png')\n",
    "    #plt.show()\n",
    "\n",
    "    # Generate synthetic data\n",
    "    num_samples = len(X_train)\n",
    "    input_data = np.random.normal(size=(num_samples, INPUT_SHAPE))\n",
    "    generated_data = autoencoder.predict(input_data)\n",
    "    reshaped_data = generated_data.reshape(num_samples, -1)\n",
    "    df_generated = pd.DataFrame(reshaped_data, columns=X_train.columns)\n",
    "\n",
    "    # Plot correlation matrices\n",
    "    corr_matrix1 = X_train.corr()\n",
    "    corr_matrix2 = df_generated.corr()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "    sns.heatmap(corr_matrix1, annot=True, cmap='coolwarm', ax=axes[0])\n",
    "    axes[0].set_title('Heatmap 1')\n",
    "\n",
    "    sns.heatmap(corr_matrix2, annot=True, cmap='coolwarm', ax=axes[1])\n",
    "    axes[1].set_title('Heatmap 2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FILE_NAME + 'HeatMaps')\n",
    "    #plt.show()\n",
    "\n",
    "    # Calculate mean and standard deviation of original and synthetic datasets\n",
    "    common_columns = set(X_train.columns) & set(df_generated.columns)\n",
    "    results = {}\n",
    "\n",
    "    for column in common_columns:\n",
    "        mean_df1 = X_train[column].mean()\n",
    "        std_df1 = X_train[column].std()\n",
    "        mean_df2 = df_generated[column].mean()\n",
    "        std_df2 = df_generated[column].std()\n",
    "\n",
    "        results[column] = {'Mean_df1': mean_df1, 'Std_df1': std_df1,\n",
    "                           'Mean_df2': mean_df2, 'Std_df2': std_df2}\n",
    "\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    comparison_df.to_csv(FILE_NAME + 'mean_std.csv', index=True)\n",
    "\n",
    "    # Plot scatter plots\n",
    "    new_index = pd.RangeIndex(start=0, stop=57, step=1)\n",
    "    X_train = X_train.set_index(new_index)\n",
    "    common_columns = set(X_train.columns) & set(df_generated.columns)\n",
    "\n",
    "    for column in common_columns:\n",
    "        plt.scatter(X_train.index, X_train[column], color='red', label='X_train')\n",
    "        plt.scatter(df_generated.index, df_generated[column], color='blue', label='df_generated')\n",
    "\n",
    "        plt.title(f\"Scatter Plot: {column}\")\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(column)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.savefig(FILE_NAME + \"_\" + column)\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "    # Add back the class label\n",
    "    X_train['class'] = 0.0\n",
    "    df_generated['class'] = 0.0\n",
    "    X_train.to_csv(FILE_NAME + 'Original_minority_data.csv', index=False)\n",
    "    df_generated.to_csv(FILE_NAME + 'Synthetic_minority_data.csv', index=False)\n",
    "\n",
    "    # Generate quality report and display it\n",
    "    real_data = FILE_NAME + 'Original_minority_data.csv'\n",
    "    synth_data = FILE_NAME + 'Synthetic_minority_data.csv'\n",
    "\n",
    "    # report = QualityReport(data_source=synth_data, ref_data=real_data)\n",
    "    # report.run()\n",
    "\n",
    "    # print(report.peek())\n",
    "    #run_summary.append([FILE_NAME, report.peek()['raw_score'], report.peek()['grade']])\n",
    "    run_summary.append([FILE_NAME, last_epoch_loss, last_epoch_val_loss])\n",
    "\n",
    "    print(\"Reports Saved\")\n",
    "    #IPython.display.HTML(report.as_html, metadata=dict(isolated=True))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b971e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def runner(encoder_dense_layers, bottle_neck, decoder_dense_layers):\n",
    "  #encoder_dense_layers = [10, 8]\n",
    "  #bottle_neck = 5\n",
    "  #decoder_dense_layers = [8, 10]\n",
    "  #FILE_NAME = f\"24_{'_'.join(map(str, encoder_dense_layers))}_{bottle_neck}_\"\n",
    "  FILE_NAME = f\"L{NO_OF_COLUMNS}_E{'_'.join(map(str, encoder_dense_layers))}_B{bottle_neck}_D{'_'.join(map(str, decoder_dense_layers))}\"   \n",
    "  report = generate_autoencoder_reports(encoder_dense_layers, bottle_neck, decoder_dense_layers)\n",
    "  #print(type(report))\n",
    "  # Save the report object to a file\n",
    "  # with open(FILE_NAME + 'quality_report.pickle', 'wb') as file:\n",
    "  #     pickle.dump(report, file)\n",
    "\n",
    "  #IPython.display.HTML(report.as_html, metadata=dict(isolated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f70bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here goes nothin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41fe338",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dense_layers_trial = [[10, 8], [12, 10], [14, 12], [16, 14], [18, 16], [20, 18],[22, 20]]\n",
    "decoder_dense_layers_trial = [[6, 8, 10, 12], [8, 10, 12, 14], [10, 12, 14, 16], [12, 14, 16, 18], [14, 16, 18, 20], \n",
    "                              [16, 18, 20, 22], [18, 20, 22, 24]]\n",
    "bottle_neck_trial = [8, 10, 12, 14, 16, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed3c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model in Pipeline: 294\n",
      "[10, 8] 8 [6, 8, 10, 12]\n",
      "[10, 8] 8 [8, 10, 12, 14]\n",
      "[10, 8] 8 [10, 12, 14, 16]\n",
      "[10, 8] 8 [12, 14, 16, 18]\n",
      "[10, 8] 8 [14, 16, 18, 20]\n",
      "[10, 8] 8 [16, 18, 20, 22]\n",
      "[10, 8] 8 [18, 20, 22, 24]\n",
      "[12, 10] 8 [6, 8, 10, 12]\n",
      "[12, 10] 8 [8, 10, 12, 14]\n",
      "[12, 10] 8 [10, 12, 14, 16]\n",
      "[12, 10] 8 [12, 14, 16, 18]\n",
      "[12, 10] 8 [14, 16, 18, 20]\n",
      "[12, 10] 8 [16, 18, 20, 22]\n",
      "[12, 10] 8 [18, 20, 22, 24]\n",
      "[14, 12] 8 [6, 8, 10, 12]\n",
      "[14, 12] 8 [8, 10, 12, 14]\n",
      "[14, 12] 8 [10, 12, 14, 16]\n",
      "[14, 12] 8 [12, 14, 16, 18]\n",
      "[14, 12] 8 [14, 16, 18, 20]\n",
      "[14, 12] 8 [16, 18, 20, 22]\n",
      "[14, 12] 8 [18, 20, 22, 24]\n",
      "[16, 14] 8 [6, 8, 10, 12]\n",
      "[16, 14] 8 [8, 10, 12, 14]\n",
      "[16, 14] 8 [10, 12, 14, 16]\n",
      "[16, 14] 8 [12, 14, 16, 18]\n",
      "[16, 14] 8 [14, 16, 18, 20]\n",
      "[16, 14] 8 [16, 18, 20, 22]\n",
      "[16, 14] 8 [18, 20, 22, 24]\n",
      "[18, 16] 8 [6, 8, 10, 12]\n",
      "[18, 16] 8 [8, 10, 12, 14]\n",
      "[18, 16] 8 [10, 12, 14, 16]\n",
      "[18, 16] 8 [12, 14, 16, 18]\n",
      "[18, 16] 8 [14, 16, 18, 20]\n",
      "[18, 16] 8 [16, 18, 20, 22]\n",
      "[18, 16] 8 [18, 20, 22, 24]\n",
      "[20, 18] 8 [6, 8, 10, 12]\n",
      "[20, 18] 8 [8, 10, 12, 14]\n",
      "[20, 18] 8 [10, 12, 14, 16]\n",
      "[20, 18] 8 [12, 14, 16, 18]\n",
      "[20, 18] 8 [14, 16, 18, 20]\n",
      "[20, 18] 8 [16, 18, 20, 22]\n",
      "[20, 18] 8 [18, 20, 22, 24]\n",
      "[22, 20] 8 [6, 8, 10, 12]\n",
      "[22, 20] 8 [8, 10, 12, 14]\n",
      "[22, 20] 8 [10, 12, 14, 16]\n",
      "[22, 20] 8 [12, 14, 16, 18]\n",
      "[22, 20] 8 [14, 16, 18, 20]\n",
      "[22, 20] 8 [16, 18, 20, 22]\n",
      "[22, 20] 8 [18, 20, 22, 24]\n",
      "[10, 8] 10 [6, 8, 10, 12]\n",
      "[10, 8] 10 [8, 10, 12, 14]\n",
      "[10, 8] 10 [10, 12, 14, 16]\n",
      "[10, 8] 10 [12, 14, 16, 18]\n",
      "[10, 8] 10 [14, 16, 18, 20]\n",
      "[10, 8] 10 [16, 18, 20, 22]\n",
      "[10, 8] 10 [18, 20, 22, 24]\n",
      "[12, 10] 10 [6, 8, 10, 12]\n",
      "[12, 10] 10 [8, 10, 12, 14]\n",
      "[12, 10] 10 [10, 12, 14, 16]\n",
      "[12, 10] 10 [12, 14, 16, 18]\n",
      "[12, 10] 10 [14, 16, 18, 20]\n",
      "[12, 10] 10 [16, 18, 20, 22]\n",
      "[12, 10] 10 [18, 20, 22, 24]\n",
      "[14, 12] 10 [6, 8, 10, 12]\n",
      "[14, 12] 10 [8, 10, 12, 14]\n",
      "[14, 12] 10 [10, 12, 14, 16]\n",
      "[14, 12] 10 [12, 14, 16, 18]\n",
      "[14, 12] 10 [14, 16, 18, 20]\n",
      "[14, 12] 10 [16, 18, 20, 22]\n",
      "[14, 12] 10 [18, 20, 22, 24]\n",
      "[16, 14] 10 [6, 8, 10, 12]\n",
      "[16, 14] 10 [8, 10, 12, 14]\n",
      "[16, 14] 10 [10, 12, 14, 16]\n",
      "[16, 14] 10 [12, 14, 16, 18]\n",
      "[16, 14] 10 [14, 16, 18, 20]\n",
      "[16, 14] 10 [16, 18, 20, 22]\n",
      "[16, 14] 10 [18, 20, 22, 24]\n",
      "[18, 16] 10 [6, 8, 10, 12]\n",
      "[18, 16] 10 [8, 10, 12, 14]\n",
      "[18, 16] 10 [10, 12, 14, 16]\n",
      "[18, 16] 10 [12, 14, 16, 18]\n",
      "[18, 16] 10 [14, 16, 18, 20]\n",
      "[18, 16] 10 [16, 18, 20, 22]\n",
      "[18, 16] 10 [18, 20, 22, 24]\n",
      "[20, 18] 10 [6, 8, 10, 12]\n",
      "[20, 18] 10 [8, 10, 12, 14]\n",
      "[20, 18] 10 [10, 12, 14, 16]\n",
      "[20, 18] 10 [12, 14, 16, 18]\n",
      "[20, 18] 10 [14, 16, 18, 20]\n",
      "[20, 18] 10 [16, 18, 20, 22]\n",
      "[20, 18] 10 [18, 20, 22, 24]\n",
      "[22, 20] 10 [6, 8, 10, 12]\n",
      "[22, 20] 10 [8, 10, 12, 14]\n",
      "[22, 20] 10 [10, 12, 14, 16]\n",
      "[22, 20] 10 [12, 14, 16, 18]\n",
      "[22, 20] 10 [14, 16, 18, 20]\n",
      "[22, 20] 10 [16, 18, 20, 22]\n",
      "[22, 20] 10 [18, 20, 22, 24]\n",
      "[10, 8] 12 [6, 8, 10, 12]\n",
      "[10, 8] 12 [8, 10, 12, 14]\n",
      "[10, 8] 12 [10, 12, 14, 16]\n",
      "[10, 8] 12 [12, 14, 16, 18]\n",
      "[10, 8] 12 [14, 16, 18, 20]\n",
      "[10, 8] 12 [16, 18, 20, 22]\n",
      "[10, 8] 12 [18, 20, 22, 24]\n",
      "[12, 10] 12 [6, 8, 10, 12]\n",
      "[12, 10] 12 [8, 10, 12, 14]\n",
      "[12, 10] 12 [10, 12, 14, 16]\n",
      "[12, 10] 12 [12, 14, 16, 18]\n",
      "[12, 10] 12 [14, 16, 18, 20]\n",
      "[12, 10] 12 [16, 18, 20, 22]\n",
      "[12, 10] 12 [18, 20, 22, 24]\n",
      "[14, 12] 12 [6, 8, 10, 12]\n",
      "[14, 12] 12 [8, 10, 12, 14]\n",
      "[14, 12] 12 [10, 12, 14, 16]\n",
      "[14, 12] 12 [12, 14, 16, 18]\n",
      "[14, 12] 12 [14, 16, 18, 20]\n",
      "[14, 12] 12 [16, 18, 20, 22]\n",
      "[14, 12] 12 [18, 20, 22, 24]\n",
      "[16, 14] 12 [6, 8, 10, 12]\n",
      "[16, 14] 12 [8, 10, 12, 14]\n",
      "[16, 14] 12 [10, 12, 14, 16]\n",
      "[16, 14] 12 [12, 14, 16, 18]\n",
      "[16, 14] 12 [14, 16, 18, 20]\n",
      "[16, 14] 12 [16, 18, 20, 22]\n",
      "[16, 14] 12 [18, 20, 22, 24]\n",
      "[18, 16] 12 [6, 8, 10, 12]\n",
      "[18, 16] 12 [8, 10, 12, 14]\n",
      "[18, 16] 12 [10, 12, 14, 16]\n",
      "[18, 16] 12 [12, 14, 16, 18]\n",
      "[18, 16] 12 [14, 16, 18, 20]\n",
      "[18, 16] 12 [16, 18, 20, 22]\n",
      "[18, 16] 12 [18, 20, 22, 24]\n",
      "[20, 18] 12 [6, 8, 10, 12]\n",
      "[20, 18] 12 [8, 10, 12, 14]\n",
      "[20, 18] 12 [10, 12, 14, 16]\n",
      "[20, 18] 12 [12, 14, 16, 18]\n",
      "[20, 18] 12 [14, 16, 18, 20]\n",
      "[20, 18] 12 [16, 18, 20, 22]\n",
      "[20, 18] 12 [18, 20, 22, 24]\n",
      "[22, 20] 12 [6, 8, 10, 12]\n",
      "[22, 20] 12 [8, 10, 12, 14]\n",
      "[22, 20] 12 [10, 12, 14, 16]\n",
      "[22, 20] 12 [12, 14, 16, 18]\n",
      "[22, 20] 12 [14, 16, 18, 20]\n",
      "[22, 20] 12 [16, 18, 20, 22]\n",
      "[22, 20] 12 [18, 20, 22, 24]\n",
      "[10, 8] 14 [6, 8, 10, 12]\n",
      "[10, 8] 14 [8, 10, 12, 14]\n",
      "[10, 8] 14 [10, 12, 14, 16]\n",
      "[10, 8] 14 [12, 14, 16, 18]\n",
      "[10, 8] 14 [14, 16, 18, 20]\n",
      "[10, 8] 14 [16, 18, 20, 22]\n",
      "[10, 8] 14 [18, 20, 22, 24]\n",
      "[12, 10] 14 [6, 8, 10, 12]\n",
      "[12, 10] 14 [8, 10, 12, 14]\n",
      "[12, 10] 14 [10, 12, 14, 16]\n",
      "[12, 10] 14 [12, 14, 16, 18]\n",
      "[12, 10] 14 [14, 16, 18, 20]\n",
      "[12, 10] 14 [16, 18, 20, 22]\n",
      "[12, 10] 14 [18, 20, 22, 24]\n",
      "[14, 12] 14 [6, 8, 10, 12]\n",
      "[14, 12] 14 [8, 10, 12, 14]\n",
      "[14, 12] 14 [10, 12, 14, 16]\n",
      "[14, 12] 14 [12, 14, 16, 18]\n",
      "[14, 12] 14 [14, 16, 18, 20]\n",
      "[14, 12] 14 [16, 18, 20, 22]\n",
      "[14, 12] 14 [18, 20, 22, 24]\n",
      "[16, 14] 14 [6, 8, 10, 12]\n",
      "[16, 14] 14 [8, 10, 12, 14]\n",
      "[16, 14] 14 [10, 12, 14, 16]\n",
      "[16, 14] 14 [12, 14, 16, 18]\n",
      "[16, 14] 14 [14, 16, 18, 20]\n",
      "[16, 14] 14 [16, 18, 20, 22]\n",
      "[16, 14] 14 [18, 20, 22, 24]\n",
      "[18, 16] 14 [6, 8, 10, 12]\n",
      "[18, 16] 14 [8, 10, 12, 14]\n",
      "[18, 16] 14 [10, 12, 14, 16]\n",
      "[18, 16] 14 [12, 14, 16, 18]\n",
      "[18, 16] 14 [14, 16, 18, 20]\n",
      "[18, 16] 14 [16, 18, 20, 22]\n",
      "[18, 16] 14 [18, 20, 22, 24]\n",
      "[20, 18] 14 [6, 8, 10, 12]\n",
      "[20, 18] 14 [8, 10, 12, 14]\n",
      "[20, 18] 14 [10, 12, 14, 16]\n",
      "[20, 18] 14 [12, 14, 16, 18]\n",
      "[20, 18] 14 [14, 16, 18, 20]\n",
      "[20, 18] 14 [16, 18, 20, 22]\n",
      "[20, 18] 14 [18, 20, 22, 24]\n",
      "[22, 20] 14 [6, 8, 10, 12]\n",
      "[22, 20] 14 [8, 10, 12, 14]\n",
      "[22, 20] 14 [10, 12, 14, 16]\n",
      "[22, 20] 14 [12, 14, 16, 18]\n",
      "[22, 20] 14 [14, 16, 18, 20]\n",
      "[22, 20] 14 [16, 18, 20, 22]\n",
      "[22, 20] 14 [18, 20, 22, 24]\n",
      "[10, 8] 16 [6, 8, 10, 12]\n",
      "[10, 8] 16 [8, 10, 12, 14]\n",
      "[10, 8] 16 [10, 12, 14, 16]\n",
      "[10, 8] 16 [12, 14, 16, 18]\n",
      "[10, 8] 16 [14, 16, 18, 20]\n",
      "[10, 8] 16 [16, 18, 20, 22]\n",
      "[10, 8] 16 [18, 20, 22, 24]\n",
      "[12, 10] 16 [6, 8, 10, 12]\n",
      "[12, 10] 16 [8, 10, 12, 14]\n",
      "[12, 10] 16 [10, 12, 14, 16]\n",
      "[12, 10] 16 [12, 14, 16, 18]\n",
      "[12, 10] 16 [14, 16, 18, 20]\n",
      "[12, 10] 16 [16, 18, 20, 22]\n",
      "[12, 10] 16 [18, 20, 22, 24]\n",
      "[14, 12] 16 [6, 8, 10, 12]\n",
      "[14, 12] 16 [8, 10, 12, 14]\n",
      "[14, 12] 16 [10, 12, 14, 16]\n",
      "[14, 12] 16 [12, 14, 16, 18]\n",
      "[14, 12] 16 [14, 16, 18, 20]\n",
      "[14, 12] 16 [16, 18, 20, 22]\n",
      "[14, 12] 16 [18, 20, 22, 24]\n",
      "[16, 14] 16 [6, 8, 10, 12]\n",
      "[16, 14] 16 [8, 10, 12, 14]\n",
      "[16, 14] 16 [10, 12, 14, 16]\n",
      "[16, 14] 16 [12, 14, 16, 18]\n",
      "[16, 14] 16 [14, 16, 18, 20]\n",
      "[16, 14] 16 [16, 18, 20, 22]\n",
      "[16, 14] 16 [18, 20, 22, 24]\n",
      "[18, 16] 16 [6, 8, 10, 12]\n",
      "[18, 16] 16 [8, 10, 12, 14]\n",
      "[18, 16] 16 [10, 12, 14, 16]\n",
      "[18, 16] 16 [12, 14, 16, 18]\n",
      "[18, 16] 16 [14, 16, 18, 20]\n",
      "[18, 16] 16 [16, 18, 20, 22]\n",
      "[18, 16] 16 [18, 20, 22, 24]\n",
      "[20, 18] 16 [6, 8, 10, 12]\n",
      "[20, 18] 16 [8, 10, 12, 14]\n",
      "[20, 18] 16 [10, 12, 14, 16]\n",
      "[20, 18] 16 [12, 14, 16, 18]\n",
      "[20, 18] 16 [14, 16, 18, 20]\n",
      "[20, 18] 16 [16, 18, 20, 22]\n",
      "[20, 18] 16 [18, 20, 22, 24]\n",
      "[22, 20] 16 [6, 8, 10, 12]\n",
      "[22, 20] 16 [8, 10, 12, 14]\n",
      "[22, 20] 16 [10, 12, 14, 16]\n",
      "[22, 20] 16 [12, 14, 16, 18]\n",
      "[22, 20] 16 [14, 16, 18, 20]\n",
      "[22, 20] 16 [16, 18, 20, 22]\n",
      "[22, 20] 16 [18, 20, 22, 24]\n",
      "[10, 8] 18 [6, 8, 10, 12]\n",
      "[10, 8] 18 [8, 10, 12, 14]\n",
      "[10, 8] 18 [10, 12, 14, 16]\n",
      "[10, 8] 18 [12, 14, 16, 18]\n",
      "[10, 8] 18 [14, 16, 18, 20]\n",
      "[10, 8] 18 [16, 18, 20, 22]\n",
      "[10, 8] 18 [18, 20, 22, 24]\n",
      "[12, 10] 18 [6, 8, 10, 12]\n",
      "[12, 10] 18 [8, 10, 12, 14]\n",
      "[12, 10] 18 [10, 12, 14, 16]\n",
      "[12, 10] 18 [12, 14, 16, 18]\n",
      "[12, 10] 18 [14, 16, 18, 20]\n",
      "[12, 10] 18 [16, 18, 20, 22]\n",
      "[12, 10] 18 [18, 20, 22, 24]\n",
      "[14, 12] 18 [6, 8, 10, 12]\n",
      "[14, 12] 18 [8, 10, 12, 14]\n",
      "[14, 12] 18 [10, 12, 14, 16]\n",
      "[14, 12] 18 [12, 14, 16, 18]\n",
      "[14, 12] 18 [14, 16, 18, 20]\n",
      "[14, 12] 18 [16, 18, 20, 22]\n",
      "[14, 12] 18 [18, 20, 22, 24]\n",
      "[16, 14] 18 [6, 8, 10, 12]\n",
      "[16, 14] 18 [8, 10, 12, 14]\n",
      "[16, 14] 18 [10, 12, 14, 16]\n",
      "[16, 14] 18 [12, 14, 16, 18]\n",
      "[16, 14] 18 [14, 16, 18, 20]\n",
      "[16, 14] 18 [16, 18, 20, 22]\n",
      "[16, 14] 18 [18, 20, 22, 24]\n",
      "[18, 16] 18 [6, 8, 10, 12]\n",
      "[18, 16] 18 [8, 10, 12, 14]\n",
      "[18, 16] 18 [10, 12, 14, 16]\n",
      "[18, 16] 18 [12, 14, 16, 18]\n",
      "[18, 16] 18 [14, 16, 18, 20]\n",
      "[18, 16] 18 [16, 18, 20, 22]\n",
      "[18, 16] 18 [18, 20, 22, 24]\n",
      "[20, 18] 18 [6, 8, 10, 12]\n",
      "[20, 18] 18 [8, 10, 12, 14]\n",
      "[20, 18] 18 [10, 12, 14, 16]\n",
      "[20, 18] 18 [12, 14, 16, 18]\n",
      "[20, 18] 18 [14, 16, 18, 20]\n",
      "[20, 18] 18 [16, 18, 20, 22]\n",
      "[20, 18] 18 [18, 20, 22, 24]\n",
      "[22, 20] 18 [6, 8, 10, 12]\n",
      "[22, 20] 18 [8, 10, 12, 14]\n",
      "[22, 20] 18 [10, 12, 14, 16]\n",
      "[22, 20] 18 [12, 14, 16, 18]\n",
      "[22, 20] 18 [14, 16, 18, 20]\n",
      "[22, 20] 18 [16, 18, 20, 22]\n",
      "[22, 20] 18 [18, 20, 22, 24]\n"
     ]
    }
   ],
   "source": [
    "total_iterations = len(bottle_neck_trial) * len(encoder_dense_layers_trial) * len(decoder_dense_layers_trial)\n",
    "print(\"Total Model in Pipeline:\",total_iterations)\n",
    "\n",
    "# Print total models\n",
    "for bn in bottle_neck_trial:\n",
    "  for enc_layers in encoder_dense_layers_trial:\n",
    "      for dec_layers in decoder_dense_layers_trial:\n",
    "          print(enc_layers, bn, dec_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd13cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Type\tBottleneck\tModel\tTraining Loss\tValidation Loss\n",
    "Heavy Decoder\tB8\tL24_E10_8_B8_D6_8_10_12\t0.019926628\t0.031914495\n",
    "Heavy Decoder\tB8\tL24_E10_8_B8_D8_10_12_14\t0.012820457\t0.020630909\n",
    "Heavy Decoder\tB8\tL24_E10_8_B8_D10_12_14_16\t0.015678098\t0.026914198\n",
    "Heavy Decoder\tB8\tL24_E10_8_B8_D12_14_16_18\t0.010084689\t0.01811916\n",
    "Heavy Decoder\tB8\tL24_E10_8_B8_D14_16_18_20\t0.013961583\t0.024084207\n",
    "Heavy Decoder\tB8\tL24_E10_8_B8_D16_18_20_22\t0.011970182\t0.019392315\n",
    "Heavy Decoder\tB8\tL24_E10_8_B8_D18_20_22_24\t0.011579308\t0.01802923\n",
    "Heavy Decoder\tB8\tL24_E12_10_B8_D6_8_10_12\t0.017853063\t0.028062725\n",
    "Heavy Decoder\tB8\tL24_E12_10_B8_D8_10_12_14\t0.011113857\t0.019850979\n",
    "Heavy Decoder\tB8\tL24_E12_10_B8_D10_12_14_16\t0.013588903\t0.021442031\n",
    "Heavy Decoder\tB8\tL24_E12_10_B8_D12_14_16_18\t0.011265877\t0.020352019\n",
    "Heavy Decoder\tB8\tL24_E12_10_B8_D14_16_18_20\t0.013636555\t0.024873422\n",
    "Heavy Decoder\tB8\tL24_E12_10_B8_D16_18_20_22\t0.011745661\t0.017587217\n",
    "Heavy Decoder\tB8\tL24_E12_10_B8_D18_20_22_24\t0.009548418\t0.016957927\n",
    "Heavy Decoder\tB8\tL24_E14_12_B8_D6_8_10_12\t0.012102078\t0.020032769\n",
    "Heavy Decoder\tB8\tL24_E14_12_B8_D8_10_12_14\t0.014465909\t0.026334645\n",
    "Heavy Decoder\tB8\tL24_E14_12_B8_D10_12_14_16\t0.014891272\t0.02552733\n",
    "Heavy Decoder\tB8\tL24_E14_12_B8_D12_14_16_18\t0.010671251\t0.017016433\n",
    "Heavy Decoder\tB8\tL24_E14_12_B8_D14_16_18_20\t0.014918462\t0.028301496\n",
    "Heavy Decoder\tB8\tL24_E14_12_B8_D16_18_20_22\t0.008329754\t0.014043828\n",
    "Heavy Decoder\tB8\tL24_E14_12_B8_D18_20_22_24\t0.009826655\t0.017809462\n",
    "Heavy Decoder\tB8\tL24_E16_14_B8_D6_8_10_12\t0.013053085\t0.019166024\n",
    "Heavy Decoder\tB8\tL24_E16_14_B8_D8_10_12_14\t0.013206534\t0.01936901\n",
    "Heavy Decoder\tB8\tL24_E16_14_B8_D10_12_14_16\t0.011198245\t0.019090597\n",
    "Heavy Decoder\tB8\tL24_E16_14_B8_D12_14_16_18\t0.009617953\t0.017003838\n",
    "Heavy Decoder\tB8\tL24_E16_14_B8_D14_16_18_20\t0.008466826\t0.013237664\n",
    "Heavy Decoder\tB8\tL24_E16_14_B8_D16_18_20_22\t0.012426714\t0.020856777\n",
    "Heavy Decoder\tB8\tL24_E16_14_B8_D18_20_22_24\t0.010172438\t0.018499367\n",
    "Heavy Decoder\tB8\tL24_E18_16_B8_D6_8_10_12\t0.012117364\t0.018034356\n",
    "Heavy Decoder\tB8\tL24_E18_16_B8_D8_10_12_14\t0.012578387\t0.0200841\n",
    "Heavy Decoder\tB8\tL24_E18_16_B8_D10_12_14_16\t0.015688391\t0.029850515\n",
    "Heavy Decoder\tB8\tL24_E18_16_B8_D12_14_16_18\t0.011237763\t0.018727092\n",
    "Heavy Decoder\tB8\tL24_E18_16_B8_D14_16_18_20\t0.008583124\t0.015225613\n",
    "Heavy Decoder\tB8\tL24_E18_16_B8_D16_18_20_22\t0.015706472\t0.02766593\n",
    "Heavy Decoder\tB8\tL24_E18_16_B8_D18_20_22_24\t0.008851178\t0.018214721\n",
    "Heavy Decoder\tB8\tL24_E20_18_B8_D6_8_10_12\t0.011311103\t0.020479875\n",
    "Heavy Decoder\tB8\tL24_E20_18_B8_D8_10_12_14\t0.018034931\t0.027819661\n",
    "Heavy Decoder\tB8\tL24_E20_18_B8_D10_12_14_16\t0.011792514\t0.018304996\n",
    "Heavy Decoder\tB8\tL24_E20_18_B8_D12_14_16_18\t0.011361422\t0.017289581\n",
    "Heavy Decoder\tB8\tL24_E20_18_B8_D14_16_18_20\t0.009899212\t0.017126333\n",
    "Heavy Decoder\tB8\tL24_E20_18_B8_D16_18_20_22\t0.010891235\t0.017784117\n",
    "Heavy Decoder\tB8\tL24_E20_18_B8_D18_20_22_24\t0.011735962\t0.019991931\n",
    "Heavy Decoder\tB8\tL24_E22_20_B8_D6_8_10_12\t0.011352683\t0.018977832\n",
    "Heavy Decoder\tB8\tL24_E22_20_B8_D8_10_12_14\t0.011352731\t0.018756287\n",
    "Heavy Decoder\tB8\tL24_E22_20_B8_D10_12_14_16\t0.012082977\t0.020539038\n",
    "Heavy Decoder\tB8\tL24_E22_20_B8_D12_14_16_18\t0.011525488\t0.018882483\n",
    "Heavy Decoder\tB8\tL24_E22_20_B8_D14_16_18_20\t0.009868192\t0.016412564\n",
    "Heavy Decoder\tB8\tL24_E22_20_B8_D16_18_20_22\t0.009056972\t0.014489492\n",
    "Heavy Decoder\tB8\tL24_E22_20_B8_D18_20_22_24\t0.007857903\t0.014609795\n",
    "Heavy Decoder\tB10\tL24_E10_8_B10_D6_8_10_12\t0.01617749\t0.024610464\n",
    "Heavy Decoder\tB10\tL24_E10_8_B10_D8_10_12_14\t0.01199057\t0.018966563\n",
    "Heavy Decoder\tB10\tL24_E10_8_B10_D10_12_14_16\t0.011335118\t0.019346694\n",
    "Heavy Decoder\tB10\tL24_E10_8_B10_D12_14_16_18\t0.011492903\t0.018522641\n",
    "Heavy Decoder\tB10\tL24_E10_8_B10_D14_16_18_20\t0.014547646\t0.033157822\n",
    "Heavy Decoder\tB10\tL24_E10_8_B10_D16_18_20_22\t0.012943004\t0.021177737\n",
    "Heavy Decoder\tB10\tL24_E10_8_B10_D18_20_22_24\t0.01279323\t0.021971969\n",
    "Heavy Decoder\tB10\tL24_E12_10_B10_D6_8_10_12\t0.020811589\t0.030136764\n",
    "Heavy Decoder\tB10\tL24_E12_10_B10_D8_10_12_14\t0.0120162\t0.018875588\n",
    "Heavy Decoder\tB10\tL24_E12_10_B10_D10_12_14_16\t0.012202449\t0.018837618\n",
    "Heavy Decoder\tB10\tL24_E12_10_B10_D12_14_16_18\t0.016870109\t0.027344983\n",
    "Heavy Decoder\tB10\tL24_E12_10_B10_D14_16_18_20\t0.011805949\t0.019425165\n",
    "Heavy Decoder\tB10\tL24_E12_10_B10_D16_18_20_22\t0.012574358\t0.020960186\n",
    "Heavy Decoder\tB10\tL24_E12_10_B10_D18_20_22_24\t0.017579434\t0.02617182\n",
    "Heavy Decoder\tB10\tL24_E14_12_B10_D6_8_10_12\t0.014620586\t0.021581242\n",
    "Heavy Decoder\tB10\tL24_E14_12_B10_D8_10_12_14\t0.012811122\t0.019523477\n",
    "Heavy Decoder\tB10\tL24_E14_12_B10_D10_12_14_16\t0.011655763\t0.019696247\n",
    "Heavy Decoder\tB10\tL24_E14_12_B10_D12_14_16_18\t0.015587794\t0.022968223\n",
    "Heavy Decoder\tB10\tL24_E14_12_B10_D14_16_18_20\t0.010810665\t0.017720209\n",
    "Heavy Decoder\tB10\tL24_E14_12_B10_D16_18_20_22\t0.010457464\t0.020521071\n",
    "Heavy Decoder\tB10\tL24_E14_12_B10_D18_20_22_24\t0.009871496\t0.01755807\n",
    "Heavy Decoder\tB10\tL24_E16_14_B10_D6_8_10_12\t0.019933959\t0.029187543\n",
    "Heavy Decoder\tB10\tL24_E16_14_B10_D8_10_12_14\t0.013325458\t0.01986823\n",
    "Heavy Decoder\tB10\tL24_E16_14_B10_D10_12_14_16\t0.008458379\t0.016088435\n",
    "Heavy Decoder\tB10\tL24_E16_14_B10_D12_14_16_18\t0.014076924\t0.024699261\n",
    "Heavy Decoder\tB10\tL24_E16_14_B10_D14_16_18_20\t0.007200642\t0.015180391\n",
    "Heavy Decoder\tB10\tL24_E16_14_B10_D16_18_20_22\t0.009967934\t0.017722603\n",
    "Heavy Decoder\tB10\tL24_E16_14_B10_D18_20_22_24\t0.011696822\t0.020563954\n",
    "Heavy Decoder\tB10\tL24_E18_16_B10_D6_8_10_12\t0.013405855\t0.020098869\n",
    "Heavy Decoder\tB10\tL24_E18_16_B10_D8_10_12_14\t0.01308763\t0.020692952\n",
    "Heavy Decoder\tB10\tL24_E18_16_B10_D10_12_14_16\t0.016480608\t0.024379078\n",
    "Heavy Decoder\tB10\tL24_E18_16_B10_D12_14_16_18\t0.012067924\t0.01809236\n",
    "Heavy Decoder\tB10\tL24_E18_16_B10_D14_16_18_20\t0.009437418\t0.018321879\n",
    "Heavy Decoder\tB10\tL24_E18_16_B10_D16_18_20_22\t0.010487803\t0.019605892\n",
    "Heavy Decoder\tB10\tL24_E18_16_B10_D18_20_22_24\t0.009581531\t0.017845809\n",
    "Heavy Decoder\tB10\tL24_E20_18_B10_D6_8_10_12\t0.013624102\t0.021669757\n",
    "Heavy Decoder\tB10\tL24_E20_18_B10_D8_10_12_14\t0.016905304\t0.023673935\n",
    "Heavy Decoder\tB10\tL24_E20_18_B10_D10_12_14_16\t0.009542614\t0.015575937\n",
    "Heavy Decoder\tB10\tL24_E20_18_B10_D12_14_16_18\t0.011156549\t0.017944233\n",
    "Heavy Decoder\tB10\tL24_E20_18_B10_D14_16_18_20\t0.01248119\t0.02218469\n",
    "Heavy Decoder\tB10\tL24_E20_18_B10_D16_18_20_22\t0.009186707\t0.01692301\n",
    "Heavy Decoder\tB10\tL24_E20_18_B10_D18_20_22_24\t0.008503783\t0.016528755\n",
    "Heavy Decoder\tB10\tL24_E22_20_B10_D6_8_10_12\t0.014513289\t0.022845794\n",
    "Heavy Decoder\tB10\tL24_E22_20_B10_D8_10_12_14\t0.01136691\t0.019605732\n",
    "Heavy Decoder\tB10\tL24_E22_20_B10_D10_12_14_16\t0.008032629\t0.013977082\n",
    "Heavy Decoder\tB10\tL24_E22_20_B10_D12_14_16_18\t0.006152914\t0.012730959\n",
    "Heavy Decoder\tB10\tL24_E22_20_B10_D14_16_18_20\t0.010961058\t0.017352784\n",
    "Heavy Decoder\tB10\tL24_E22_20_B10_D16_18_20_22\t0.008299788\t0.013920601\n",
    "Heavy Decoder\tB10\tL24_E22_20_B10_D18_20_22_24\t0.010940257\t0.017491288\n",
    "Heavy Decoder\tB12\tL24_E10_8_B12_D6_8_10_12\t0.02085226\t0.02993325\n",
    "Heavy Decoder\tB12\tL24_E10_8_B12_D8_10_12_14\t0.020580277\t0.03002814\n",
    "Heavy Decoder\tB12\tL24_E10_8_B12_D10_12_14_16\t0.012200225\t0.020292405\n",
    "Heavy Decoder\tB12\tL24_E10_8_B12_D12_14_16_18\t0.010822658\t0.018489391\n",
    "Heavy Decoder\tB12\tL24_E10_8_B12_D14_16_18_20\t0.011028523\t0.02141482\n",
    "Heavy Decoder\tB12\tL24_E10_8_B12_D16_18_20_22\t0.011975507\t0.019119505\n",
    "Heavy Decoder\tB12\tL24_E10_8_B12_D18_20_22_24\t0.012728666\t0.018916126\n",
    "Heavy Decoder\tB12\tL24_E12_10_B12_D6_8_10_12\t0.014082247\t0.020918448\n",
    "Heavy Decoder\tB12\tL24_E12_10_B12_D8_10_12_14\t0.01307523\t0.019432681\n",
    "Heavy Decoder\tB12\tL24_E12_10_B12_D10_12_14_16\t0.01389892\t0.023466131\n",
    "Heavy Decoder\tB12\tL24_E12_10_B12_D12_14_16_18\t0.012360448\t0.019326817\n",
    "Heavy Decoder\tB12\tL24_E12_10_B12_D14_16_18_20\t0.01406364\t0.021917408\n",
    "Heavy Decoder\tB12\tL24_E12_10_B12_D16_18_20_22\t0.007908826\t0.014001522\n",
    "Heavy Decoder\tB12\tL24_E12_10_B12_D18_20_22_24\t0.009199761\t0.01670053\n",
    "Heavy Decoder\tB12\tL24_E14_12_B12_D6_8_10_12\t0.015532128\t0.025982391\n",
    "Heavy Decoder\tB12\tL24_E14_12_B12_D8_10_12_14\t0.012011804\t0.019856552\n",
    "Heavy Decoder\tB12\tL24_E14_12_B12_D10_12_14_16\t0.01212867\t0.019433295\n",
    "Heavy Decoder\tB12\tL24_E14_12_B12_D12_14_16_18\t0.012212859\t0.019038336\n",
    "Heavy Decoder\tB12\tL24_E14_12_B12_D14_16_18_20\t0.010627808\t0.017710555\n",
    "Heavy Decoder\tB12\tL24_E14_12_B12_D16_18_20_22\t0.00912258\t0.017804381\n",
    "Heavy Decoder\tB12\tL24_E14_12_B12_D18_20_22_24\t0.011081631\t0.018871486\n",
    "Heavy Decoder\tB12\tL24_E16_14_B12_D6_8_10_12\t0.017017398\t0.02913595\n",
    "Heavy Decoder\tB12\tL24_E16_14_B12_D8_10_12_14\t0.011995218\t0.019841345\n",
    "Heavy Decoder\tB12\tL24_E16_14_B12_D10_12_14_16\t0.008422054\t0.015579781\n",
    "Heavy Decoder\tB12\tL24_E16_14_B12_D12_14_16_18\t0.011621462\t0.019679062\n",
    "Heavy Decoder\tB12\tL24_E16_14_B12_D14_16_18_20\t0.008911361\t0.018223668\n",
    "Heavy Decoder\tB12\tL24_E16_14_B12_D16_18_20_22\t0.010081922\t0.019315099\n",
    "Heavy Decoder\tB12\tL24_E16_14_B12_D18_20_22_24\t0.008505468\t0.015559361\n",
    "Heavy Decoder\tB12\tL24_E18_16_B12_D6_8_10_12\t0.01224714\t0.018210115\n",
    "Heavy Decoder\tB12\tL24_E18_16_B12_D8_10_12_14\t0.014781563\t0.02319755\n",
    "Heavy Decoder\tB12\tL24_E18_16_B12_D10_12_14_16\t0.009717763\t0.018853044\n",
    "Heavy Decoder\tB12\tL24_E18_16_B12_D12_14_16_18\t0.013606529\t0.019454964\n",
    "Heavy Decoder\tB12\tL24_E18_16_B12_D14_16_18_20\t0.014664819\t0.023023386\n",
    "Heavy Decoder\tB12\tL24_E18_16_B12_D16_18_20_22\t0.007571518\t0.014123177\n",
    "Heavy Decoder\tB12\tL24_E18_16_B12_D18_20_22_24\t0.009623448\t0.016357137\n",
    "Heavy Decoder\tB12\tL24_E20_18_B12_D6_8_10_12\t0.012537383\t0.019836985\n",
    "Heavy Decoder\tB12\tL24_E20_18_B12_D8_10_12_14\t0.012002917\t0.018898582\n",
    "Heavy Decoder\tB12\tL24_E20_18_B12_D10_12_14_16\t0.015654644\t0.027118146\n",
    "Heavy Decoder\tB12\tL24_E20_18_B12_D12_14_16_18\t0.009979123\t0.015078695\n",
    "Heavy Decoder\tB12\tL24_E20_18_B12_D14_16_18_20\t0.011016244\t0.01805836\n",
    "Heavy Decoder\tB12\tL24_E20_18_B12_D16_18_20_22\t0.007649989\t0.013232651\n",
    "Heavy Decoder\tB12\tL24_E20_18_B12_D18_20_22_24\t0.010399478\t0.017442474\n",
    "Heavy Decoder\tB12\tL24_E22_20_B12_D6_8_10_12\t0.017334213\t0.027128486\n",
    "Heavy Decoder\tB12\tL24_E22_20_B12_D8_10_12_14\t0.013127338\t0.020654367\n",
    "Heavy Decoder\tB12\tL24_E22_20_B12_D10_12_14_16\t0.008563383\t0.013469688\n",
    "Heavy Decoder\tB12\tL24_E22_20_B12_D12_14_16_18\t0.011292834\t0.018262001\n",
    "Heavy Decoder\tB12\tL24_E22_20_B12_D14_16_18_20\t0.011746749\t0.018651644\n",
    "Heavy Decoder\tB12\tL24_E22_20_B12_D16_18_20_22\t0.009583009\t0.017767536\n",
    "Heavy Decoder\tB12\tL24_E22_20_B12_D18_20_22_24\t0.009587358\t0.018937884\n",
    "Heavy Decoder\tB14\tL24_E10_8_B14_D6_8_10_12\t0.013399748\t0.019942623\n",
    "Heavy Decoder\tB14\tL24_E10_8_B14_D8_10_12_14\t0.013856885\t0.024949348\n",
    "Heavy Decoder\tB14\tL24_E10_8_B14_D10_12_14_16\t0.012476523\t0.01968432\n",
    "Heavy Decoder\tB14\tL24_E10_8_B14_D12_14_16_18\t0.012295647\t0.01982256\n",
    "Heavy Decoder\tB14\tL24_E10_8_B14_D14_16_18_20\t0.011350309\t0.018674087\n",
    "Heavy Decoder\tB14\tL24_E10_8_B14_D16_18_20_22\t0.011908285\t0.019959204\n",
    "Heavy Decoder\tB14\tL24_E10_8_B14_D18_20_22_24\t0.011623992\t0.017855722\n",
    "Heavy Decoder\tB14\tL24_E12_10_B14_D6_8_10_12\t0.020620164\t0.030022955\n",
    "Heavy Decoder\tB14\tL24_E12_10_B14_D8_10_12_14\t0.012618531\t0.019855125\n",
    "Heavy Decoder\tB14\tL24_E12_10_B14_D10_12_14_16\t0.012234853\t0.017830957\n",
    "Heavy Decoder\tB14\tL24_E12_10_B14_D12_14_16_18\t0.008691774\t0.015750788\n",
    "Heavy Decoder\tB14\tL24_E12_10_B14_D14_16_18_20\t0.01528099\t0.025790023\n",
    "Heavy Decoder\tB14\tL24_E12_10_B14_D16_18_20_22\t0.01659053\t0.028817531\n",
    "Heavy Decoder\tB14\tL24_E12_10_B14_D18_20_22_24\t0.009748182\t0.018525895\n",
    "Heavy Decoder\tB14\tL24_E14_12_B14_D6_8_10_12\t0.015544613\t0.023030173\n",
    "Heavy Decoder\tB14\tL24_E14_12_B14_D8_10_12_14\t0.012619214\t0.020888973\n",
    "Heavy Decoder\tB14\tL24_E14_12_B14_D10_12_14_16\t0.01612485\t0.027219275\n",
    "Heavy Decoder\tB14\tL24_E14_12_B14_D12_14_16_18\t0.011722335\t0.019752827\n",
    "Heavy Decoder\tB14\tL24_E14_12_B14_D14_16_18_20\t0.010903643\t0.02102687\n",
    "Heavy Decoder\tB14\tL24_E14_12_B14_D16_18_20_22\t0.009975309\t0.01778199\n",
    "Heavy Decoder\tB14\tL24_E14_12_B14_D18_20_22_24\t0.008371046\t0.014563169\n",
    "Heavy Decoder\tB14\tL24_E16_14_B14_D6_8_10_12\t0.016914401\t0.026733216\n",
    "Heavy Decoder\tB14\tL24_E16_14_B14_D8_10_12_14\t0.020243434\t0.029316984\n",
    "Heavy Decoder\tB14\tL24_E16_14_B14_D10_12_14_16\t0.011177752\t0.019731807\n",
    "Heavy Decoder\tB14\tL24_E16_14_B14_D12_14_16_18\t0.011791554\t0.018150209\n",
    "Heavy Decoder\tB14\tL24_E16_14_B14_D14_16_18_20\t0.007874776\t0.013618091\n",
    "Heavy Decoder\tB14\tL24_E16_14_B14_D16_18_20_22\t0.011639429\t0.018968469\n",
    "Heavy Decoder\tB14\tL24_E16_14_B14_D18_20_22_24\t0.012033977\t0.018575193\n",
    "Heavy Decoder\tB14\tL24_E18_16_B14_D6_8_10_12\t0.012369959\t0.019322069\n",
    "Heavy Decoder\tB14\tL24_E18_16_B14_D8_10_12_14\t0.011988897\t0.020933587\n",
    "Heavy Decoder\tB14\tL24_E18_16_B14_D10_12_14_16\t0.010859053\t0.01884171\n",
    "Heavy Decoder\tB14\tL24_E18_16_B14_D12_14_16_18\t0.011526885\t0.027072664\n",
    "Heavy Decoder\tB14\tL24_E18_16_B14_D14_16_18_20\t0.009165065\t0.015636789\n",
    "Heavy Decoder\tB14\tL24_E18_16_B14_D16_18_20_22\t0.008879378\t0.017426174\n",
    "Heavy Decoder\tB14\tL24_E18_16_B14_D18_20_22_24\t0.008176344\t0.014083626\n",
    "Heavy Decoder\tB14\tL24_E20_18_B14_D6_8_10_12\t0.008795007\t0.014211965\n",
    "Heavy Decoder\tB14\tL24_E20_18_B14_D8_10_12_14\t0.013111577\t0.019819636\n",
    "Heavy Decoder\tB14\tL24_E20_18_B14_D10_12_14_16\t0.01173713\t0.018903952\n",
    "Heavy Decoder\tB14\tL24_E20_18_B14_D12_14_16_18\t0.011669183\t0.017639371\n",
    "Heavy Decoder\tB14\tL24_E20_18_B14_D14_16_18_20\t0.012137257\t0.018146161\n",
    "Heavy Decoder\tB14\tL24_E20_18_B14_D16_18_20_22\t0.006699539\t0.013598016\n",
    "Heavy Decoder\tB14\tL24_E20_18_B14_D18_20_22_24\t0.00774019\t0.013631775\n",
    "Heavy Decoder\tB14\tL24_E22_20_B14_D6_8_10_12\t0.011925475\t0.020186467\n",
    "Heavy Decoder\tB14\tL24_E22_20_B14_D8_10_12_14\t0.010796593\t0.018070003\n",
    "Heavy Decoder\tB14\tL24_E22_20_B14_D10_12_14_16\t0.011827203\t0.018339898\n",
    "Heavy Decoder\tB14\tL24_E22_20_B14_D12_14_16_18\t0.01117596\t0.017975174\n",
    "Heavy Decoder\tB14\tL24_E22_20_B14_D14_16_18_20\t0.006781273\t0.013291588\n",
    "Heavy Decoder\tB14\tL24_E22_20_B14_D16_18_20_22\t0.011013284\t0.017925305\n",
    "Heavy Decoder\tB14\tL24_E22_20_B14_D18_20_22_24\t0.008249566\t0.016104965\n",
    "Heavy Decoder\tB16\tL24_E10_8_B16_D6_8_10_12\t0.019243199\t0.031772904\n",
    "Heavy Decoder\tB16\tL24_E10_8_B16_D8_10_12_14\t0.008971762\t0.01557119\n",
    "Heavy Decoder\tB16\tL24_E10_8_B16_D10_12_14_16\t0.017910106\t0.026622515\n",
    "Heavy Decoder\tB16\tL24_E10_8_B16_D12_14_16_18\t0.017932832\t0.027617322\n",
    "Heavy Decoder\tB16\tL24_E10_8_B16_D14_16_18_20\t0.015073754\t0.023402266\n",
    "Heavy Decoder\tB16\tL24_E10_8_B16_D16_18_20_22\t0.01253855\t0.020166595\n",
    "Heavy Decoder\tB16\tL24_E10_8_B16_D18_20_22_24\t0.011598252\t0.02000675\n",
    "Heavy Decoder\tB16\tL24_E12_10_B16_D6_8_10_12\t0.016252495\t0.023979126\n",
    "Heavy Decoder\tB16\tL24_E12_10_B16_D8_10_12_14\t0.011826775\t0.019730644\n",
    "Heavy Decoder\tB16\tL24_E12_10_B16_D10_12_14_16\t0.010826796\t0.018572234\n",
    "Heavy Decoder\tB16\tL24_E12_10_B16_D12_14_16_18\t0.011876215\t0.019075425\n",
    "Heavy Decoder\tB16\tL24_E12_10_B16_D14_16_18_20\t0.010589904\t0.020410037\n",
    "Heavy Decoder\tB16\tL24_E12_10_B16_D16_18_20_22\t0.010889174\t0.018488754\n",
    "Heavy Decoder\tB16\tL24_E12_10_B16_D18_20_22_24\t0.006699296\t0.013336871\n",
    "Heavy Decoder\tB16\tL24_E14_12_B16_D6_8_10_12\t0.009549412\t0.015883701\n",
    "Heavy Decoder\tB16\tL24_E14_12_B16_D8_10_12_14\t0.018290259\t0.028182514\n",
    "Heavy Decoder\tB16\tL24_E14_12_B16_D10_12_14_16\t0.013240386\t0.030265464\n",
    "Heavy Decoder\tB16\tL24_E14_12_B16_D12_14_16_18\t0.01175663\t0.018449938\n",
    "Heavy Decoder\tB16\tL24_E14_12_B16_D14_16_18_20\t0.008869783\t0.015821282\n",
    "Heavy Decoder\tB16\tL24_E14_12_B16_D16_18_20_22\t0.011068586\t0.018506834\n",
    "Heavy Decoder\tB16\tL24_E14_12_B16_D18_20_22_24\t0.010057555\t0.0169142\n",
    "Heavy Decoder\tB16\tL24_E16_14_B16_D6_8_10_12\t0.014730976\t0.02505178\n",
    "Heavy Decoder\tB16\tL24_E16_14_B16_D8_10_12_14\t0.011346896\t0.023730354\n",
    "Heavy Decoder\tB16\tL24_E16_14_B16_D10_12_14_16\t0.017954346\t0.029199997\n",
    "Heavy Decoder\tB16\tL24_E16_14_B16_D12_14_16_18\t0.008034728\t0.014146945\n",
    "Heavy Decoder\tB16\tL24_E16_14_B16_D14_16_18_20\t0.008083167\t0.016773816\n",
    "Heavy Decoder\tB16\tL24_E16_14_B16_D16_18_20_22\t0.008938136\t0.018586945\n",
    "Heavy Decoder\tB16\tL24_E16_14_B16_D18_20_22_24\t0.010605593\t0.017534126\n",
    "Heavy Decoder\tB16\tL24_E18_16_B16_D6_8_10_12\t0.010811215\t0.016296335\n",
    "Heavy Decoder\tB16\tL24_E18_16_B16_D8_10_12_14\t0.011759428\t0.019265169\n",
    "Heavy Decoder\tB16\tL24_E18_16_B16_D10_12_14_16\t0.008685778\t0.016427549\n",
    "Heavy Decoder\tB16\tL24_E18_16_B16_D12_14_16_18\t0.010964803\t0.0198633\n",
    "Heavy Decoder\tB16\tL24_E18_16_B16_D14_16_18_20\t0.00823427\t0.014127891\n",
    "Heavy Decoder\tB16\tL24_E18_16_B16_D16_18_20_22\t0.007063529\t0.015473473\n",
    "Heavy Decoder\tB16\tL24_E18_16_B16_D18_20_22_24\t0.006971302\t0.013990013\n",
    "Heavy Decoder\tB16\tL24_E20_18_B16_D6_8_10_12\t0.013803542\t0.020538185\n",
    "Heavy Decoder\tB16\tL24_E20_18_B16_D8_10_12_14\t0.009578071\t0.018715315\n",
    "Heavy Decoder\tB16\tL24_E20_18_B16_D10_12_14_16\t0.01197136\t0.019383006\n",
    "Heavy Decoder\tB16\tL24_E20_18_B16_D12_14_16_18\t0.008608559\t0.015149264\n",
    "Heavy Decoder\tB16\tL24_E20_18_B16_D14_16_18_20\t0.007759089\t0.013330377\n",
    "Heavy Decoder\tB16\tL24_E20_18_B16_D16_18_20_22\t0.007215609\t0.013727581\n",
    "Heavy Decoder\tB16\tL24_E20_18_B16_D18_20_22_24\t0.009972167\t0.018072648\n",
    "Heavy Decoder\tB16\tL24_E22_20_B16_D6_8_10_12\t0.015576638\t0.02483651\n",
    "Heavy Decoder\tB16\tL24_E22_20_B16_D8_10_12_14\t0.011603369\t0.018872503\n",
    "Heavy Decoder\tB16\tL24_E22_20_B16_D10_12_14_16\t0.011327572\t0.018381715\n",
    "Heavy Decoder\tB16\tL24_E22_20_B16_D12_14_16_18\t0.007796038\t0.014864609\n",
    "Heavy Decoder\tB16\tL24_E22_20_B16_D14_16_18_20\t0.010966417\t0.022019885\n",
    "Heavy Decoder\tB16\tL24_E22_20_B16_D16_18_20_22\t0.010111888\t0.017296817\n",
    "Heavy Decoder\tB16\tL24_E22_20_B16_D18_20_22_24\t0.006754107\t0.014231541\n",
    "Heavy Decoder\tB18\tL24_E10_8_B18_D6_8_10_12\t0.018976871\t0.029170567\n",
    "Heavy Decoder\tB18\tL24_E10_8_B18_D8_10_12_14\t0.014238815\t0.022090834\n",
    "Heavy Decoder\tB18\tL24_E10_8_B18_D10_12_14_16\t0.012742657\t0.021035785\n",
    "Heavy Decoder\tB18\tL24_E10_8_B18_D12_14_16_18\t0.013809389\t0.021798797\n",
    "Heavy Decoder\tB18\tL24_E10_8_B18_D14_16_18_20\t0.017042201\t0.026084993\n",
    "Heavy Decoder\tB18\tL24_E10_8_B18_D16_18_20_22\t0.011481716\t0.018979775\n",
    "Heavy Decoder\tB18\tL24_E10_8_B18_D18_20_22_24\t0.011351201\t0.018671993\n",
    "Heavy Decoder\tB18\tL24_E12_10_B18_D6_8_10_12\t0.012793745\t0.019026058\n",
    "Heavy Decoder\tB18\tL24_E12_10_B18_D8_10_12_14\t0.009171666\t0.014108793\n",
    "Heavy Decoder\tB18\tL24_E12_10_B18_D10_12_14_16\t0.011262167\t0.018913176\n",
    "Heavy Decoder\tB18\tL24_E12_10_B18_D12_14_16_18\t0.014338922\t0.02379662\n",
    "Heavy Decoder\tB18\tL24_E12_10_B18_D14_16_18_20\t0.010038973\t0.017032314\n",
    "Heavy Decoder\tB18\tL24_E12_10_B18_D16_18_20_22\t0.009421322\t0.016635112\n",
    "Heavy Decoder\tB18\tL24_E12_10_B18_D18_20_22_24\t0.011466952\t0.018238211\n",
    "Heavy Decoder\tB18\tL24_E14_12_B18_D6_8_10_12\t0.013638024\t0.024404278\n",
    "Heavy Decoder\tB18\tL24_E14_12_B18_D8_10_12_14\t0.011867948\t0.019307017\n",
    "Heavy Decoder\tB18\tL24_E14_12_B18_D10_12_14_16\t0.012551965\t0.019245127\n",
    "Heavy Decoder\tB18\tL24_E14_12_B18_D12_14_16_18\t0.011492564\t0.018114243\n",
    "Heavy Decoder\tB18\tL24_E14_12_B18_D14_16_18_20\t0.011730838\t0.019253414\n",
    "Heavy Decoder\tB18\tL24_E14_12_B18_D16_18_20_22\t0.011584658\t0.019614082\n",
    "Heavy Decoder\tB18\tL24_E14_12_B18_D18_20_22_24\t0.007231295\t0.013612901\n",
    "Heavy Decoder\tB18\tL24_E16_14_B18_D6_8_10_12\t0.020331744\t0.029354354\n",
    "Heavy Decoder\tB18\tL24_E16_14_B18_D8_10_12_14\t0.015102516\t0.024472747\n",
    "Heavy Decoder\tB18\tL24_E16_14_B18_D10_12_14_16\t0.009042187\t0.016038889\n",
    "Heavy Decoder\tB18\tL24_E16_14_B18_D12_14_16_18\t0.012412716\t0.018926779\n",
    "Heavy Decoder\tB18\tL24_E16_14_B18_D14_16_18_20\t0.011112468\t0.018418532\n",
    "Heavy Decoder\tB18\tL24_E16_14_B18_D16_18_20_22\t0.00858915\t0.014386066\n",
    "Heavy Decoder\tB18\tL24_E16_14_B18_D18_20_22_24\t0.009409947\t0.016922887\n",
    "Heavy Decoder\tB18\tL24_E18_16_B18_D6_8_10_12\t0.019023109\t0.029412845\n",
    "Heavy Decoder\tB18\tL24_E18_16_B18_D8_10_12_14\t0.019915041\t0.029038353\n",
    "Heavy Decoder\tB18\tL24_E18_16_B18_D10_12_14_16\t0.012730284\t0.019514812\n",
    "Heavy Decoder\tB18\tL24_E18_16_B18_D12_14_16_18\t0.011264757\t0.019126305\n",
    "Heavy Decoder\tB18\tL24_E18_16_B18_D14_16_18_20\t0.007170556\t0.014943301\n",
    "Heavy Decoder\tB18\tL24_E18_16_B18_D16_18_20_22\t0.007851245\t0.014734412\n",
    "Heavy Decoder\tB18\tL24_E18_16_B18_D18_20_22_24\t0.00659407\t0.013850044\n",
    "Heavy Decoder\tB18\tL24_E20_18_B18_D6_8_10_12\t0.017369058\t0.027001059\n",
    "Heavy Decoder\tB18\tL24_E20_18_B18_D8_10_12_14\t0.013318332\t0.022722023\n",
    "Heavy Decoder\tB18\tL24_E20_18_B18_D10_12_14_16\t0.007617832\t0.014516997\n",
    "Heavy Decoder\tB18\tL24_E20_18_B18_D12_14_16_18\t0.008705925\t0.016173838\n",
    "Heavy Decoder\tB18\tL24_E20_18_B18_D14_16_18_20\t0.008279722\t0.014832592\n",
    "Heavy Decoder\tB18\tL24_E20_18_B18_D16_18_20_22\t0.007010866\t0.014778253\n",
    "Heavy Decoder\tB18\tL24_E20_18_B18_D18_20_22_24\t0.006486921\t0.013481108\n",
    "Heavy Decoder\tB18\tL24_E22_20_B18_D6_8_10_12\t0.013460875\t0.022215158\n",
    "Heavy Decoder\tB18\tL24_E22_20_B18_D8_10_12_14\t0.011536745\t0.020229192\n",
    "Heavy Decoder\tB18\tL24_E22_20_B18_D10_12_14_16\t0.009257151\t0.01637112\n",
    "Heavy Decoder\tB18\tL24_E22_20_B18_D12_14_16_18\t0.010352857\t0.017710311\n",
    "Heavy Decoder\tB18\tL24_E22_20_B18_D14_16_18_20\t0.006820044\t0.013460219\n",
    "Heavy Decoder\tB18\tL24_E22_20_B18_D16_18_20_22\t0.007571639\t0.013692248\n",
    "Heavy Decoder\tB18\tL24_E22_20_B18_D18_20_22_24\t0.008189476\t0.017249566\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
