{"cells":[{"cell_type":"markdown","metadata":{},"source":["### 1. Imports and File Paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_km2QLOdV3f"},"outputs":[],"source":["import os\n","import sys\n","import joblib\n","import numpy as np\n","from keras.models import load_model\n","from sklearn import metrics\n","import pandas as pd\n","import shutil\n","from pathlib import Path\n","from tqdm import tqdm\n","import itertools\n","from TL_BaseModel import *\n","\n","# dirpath = \"./../../../Output_Results/TL_BaseModel_Results/\"\n","# if os.path.exists(dirpath) and os.path.isdir(dirpath):\n","#     shutil.rmtree(dirpath)\n","# os.makedirs(os.path.dirname(dirpath), exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#\"#../../../../Output_Results/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrsrjYdL5r7Q"},"outputs":[],"source":["TL_path = \"../../../../Output_Results/Freeze/\"\n","Base_path = \"../../../../Output_Results/classification_results_2023_base_tl/\"\n","output_path = \"../../../../Output_Results/TL_BaseModel_Results/\"\n","\n","print(\"TL_path\",TL_path)\n","print(\"Base_path\",Base_path)\n","print(\"output_path\",output_path)"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Patients Target Group"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2knIjPxbSQd"},"outputs":[],"source":["target_grp = [\"SB-001\", \"SB-003\", \"SB-008\", \"SB-011\", \"SB-012\", \"SB-022\", \"SB-023\", \"SB-025\", \"SB-028\", \"SB-037\", \"SB-043\", \"SB-056\", \"SB-059\", \"SB-060\", \"SB-070\", \"SB-071\", \"SB-073\", \"SB-078\", \"SB-079\", \"SB-080\", \"SB-082\", \"SB-083\", \"SB-112\"]\n","print(len(target_grp))"]},{"cell_type":"markdown","metadata":{},"source":["### 3. find the best TL model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YT_8Zxyh4uGt"},"outputs":[],"source":["# Get all TL models\n","def find_best_TL_model(target_grp, TL_path, output_path, epoch_sizes = None):\n","    Model_List = []\n","    TL_folder_List = os.listdir(TL_path)\n","    # print(TL_folder_List)\n","\n","    if epoch_sizes is None:\n","        epoch_sizes = [f\"{i}_{j}\" for i in [100, 300, 500, 1000] for j in [100, 300, 500, 1000]]\n","#Output_Results/TL_results/1_layerFrozen.png/diagrams/SB-001/\n","#Output_Results\\TL_results\\TL_2_Frozen\\diagrams\\SB-112\n","    for epoch_size in epoch_sizes:\n","        for target_id in target_grp:\n","            for folder_name in TL_folder_List:\n","                TL_model_path = f\"{TL_path}/{folder_name}/diagrams/{target_id}/\"\n","                #print(\"CURRENT PATH : \",os.listdir(TL_path))\n","                #continue\n","                for file_name in os.listdir(TL_model_path):\n","                    if file_name[-5:] == \".hdf5\" and \"IND_FOLD\" in file_name and epoch_size in file_name: # Model Files with extension hdf5 #  Keyword \"IND_FOLD\" for models to be considered for TL+Base Modelling\n","                        # print(\"file_name\", file_name)\n","                        Model_List.append(file_name)\n","                        tl_model_path_file = TL_model_path + file_name\n","                        output_path_file = f\"{output_path}{epoch_size}/{target_id}/{file_name}\"\n","                        # print(\"tl_model_path_file\", tl_model_path_file)\n","                        # print(\"output_path_file\", output_path_file)\n","                        os.makedirs(os.path.dirname(f\"{output_path}{epoch_size}/{target_id}/\"), exist_ok=True)\n","                        shutil.copyfile(tl_model_path_file, output_path_file)\n","    return Model_List\n","\n","epoch_sizes = [\"100_300\"] # Best Epoch parameter as per findings during the research\n","TL_Models = find_best_TL_model(target_grp, TL_path, output_path, epoch_sizes)\n","display(TL_Models[:5], TL_Models[-5:])"]},{"cell_type":"markdown","metadata":{"id":"MatC8FtnEfCN"},"source":["### 4. find the best Base model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get all Base models\n","def find_best_Base_model(Base_path, output_path, epoch_sizes = None):\n","    Model_List = []\n","\n","    if epoch_sizes is None:\n","        epoch_sizes = [f\"{i}_{j}\" for i in [100, 300, 500, 1000] for j in [100, 300, 500, 1000]]\n","    target_grp = os.listdir(Base_path)\n","    Base_model_Folder_List = os.listdir(f\"{Base_path}{os.listdir(Base_path)[0]}/\")\n","\n","    for epoch_size in epoch_sizes:\n","        for target_id in target_grp:\n","            for folder_name in Base_model_Folder_List:\n","                file_list = os.listdir(f\"{Base_path}{target_id}/{folder_name}/diagrams/\")\n","                for file_name in file_list:\n","                    if file_name[-7:] == \".joblib\":\n","                        # display(file_name)\n","                        Model_List.append(file_name)\n","                        base_model_path_file = f\"{Base_path}{target_id}/{folder_name}/diagrams/{file_name}\"\n","                        output_path_file = f\"{output_path}{epoch_size}/{target_id}/{file_name}\"\n","                        os.makedirs(os.path.dirname(f\"{output_path}{epoch_size}/{target_id}/\"), exist_ok=True)\n","                        shutil.copyfile(base_model_path_file, output_path_file)\n","    return Model_List\n","\n","epoch_sizes = [\"100_300\"] # Best Epoch parameter as per findings during the research\n","Base_Models = find_best_Base_model(Base_path, output_path, epoch_sizes)\n","display(Base_Models[:5], Base_Models[-5:])"]},{"cell_type":"markdown","metadata":{"id":"42TWazrKEl5M"},"source":["### 5. TL + Base model"]},{"cell_type":"markdown","metadata":{},"source":["#### 5a. Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#GLOBALS AND FILEPATHS#\n","source_quantile = \"0.3\"\n","source_file_path = f\"all_pids_q={source_quantile}.npy\"\n","print(source_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"elapsed":161,"status":"ok","timestamp":1692177551142,"user":{"displayName":"Madhu Roopa","userId":"18392602879319607573"},"user_tz":420},"id":"IaQjMrVYtaAt","outputId":"17f9c43c-bff1-4ef8-a1f3-5356f76a53f9"},"outputs":[],"source":["##### KEY TROUBLESHOOTING NOTE\n","##### `np.load` returns a `numpy ndarray`\n","data_ndarr = np.load(source_file_path, allow_pickle=True)\n","print(\"data_ndarr type :\", type(data_ndarr))\n","\n","##### That array has a method `item()`,\n","##### which returns a *dictionary* of key-values: `p_id`:`DataFrame`\n","datadict = data_ndarr.item()\n","print(\"datadict type :\", type(datadict))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":150,"status":"ok","timestamp":1692177551143,"user":{"displayName":"Madhu Roopa","userId":"18392602879319607573"},"user_tz":420},"id":"yBu0kU8yFCp1","outputId":"1b97f353-85c4-4b64-d81e-390d8163a49c"},"outputs":[],"source":["p_ids = datadict.keys()\n","sample_size_dict={}\n","print(\"patients: n =\", len(p_ids), end='\\n\\n')\n","\n","for p_id in p_ids:\n","    df = datadict[p_id]\n","    # drop column from input data file\n","    df.drop(['id','date','am_pef_org','BMI','sex', 'age'], axis=1, inplace=True)\n","    df = np.asarray(df).astype(np.float32)\n","    print(p_id, \"shape:\", df.shape)#, end=\"\\t\\t\")\n","    sample_size_dict[p_id] = df.shape[0]\n","\n","display(datadict[p_id].head(5))\n","display(sample_size_dict)"]},{"cell_type":"markdown","metadata":{},"source":["#### 5b. TL and Base Models Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def TL_and_Base_Model(target_id, TL_Models, Base_Models, model_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes = None):\n","    df_evaluation_results = pd.DataFrame(columns=EVALUATION_METRICS)\n","    # Mark: EXPLICITLY set binary outcome variable for supervised learning\n","    CLASS_VAR = 'class'\n","    sample_size=sample_size_dict[target_id]\n","\n","    # load target patient data\n","    dataset_list = split_data(target=target_id, data=datadict, class_var=CLASS_VAR, balance=True)\n","\n","    if epoch_sizes is None :\n","        epoch_sizes = [f\"{i}_{j}\" for i in [100, 300, 500, 1000] for j in [100, 300, 500, 1000]]\n","\n","    for epoch_size in epoch_sizes:\n","        for Base_model_Category, tl_arch_type in itertools.product(Base_model_Category_list, tl_arch_list):\n","            # Creating Category directory for the patient\n","            os.makedirs(os.path.dirname(f\"{model_path}{epoch_size}/{target_id}/{Base_model_Category}/\"), exist_ok=True)\n","            for Base_Model in Base_Models:\n","                if Base_Model[-7:] == \".joblib\" and target_id in Base_Model and Base_model_Category in Base_Model:\n","                    # print(\"Base_Model\", Base_Model)\n","                    for TL_model in TL_Models:\n","                        # setup for k-fold cross-validation\n","                        kfold_evaluation_results_list = []\n","                        kfold_confusion_matrix_list = []\n","                        # print(\"TL_model\", TL_model)\n","                        if TL_model[-5:] == \".hdf5\" and TL_model[:6] == target_id and tl_arch_type in TL_model and epoch_size in TL_model:\n","                            TL_model_path_file = f\"{model_path}{epoch_size}/{target_id}/{TL_model}\"\n","                            # print(\"TL_model_path_file\", TL_model_path_file)\n","\n","                            for i in range(3):\n","                                print(f'================================================{i}================================================')\n","                                X_train, X_test, Y_train, Y_test = dataset_list[i]\n","\n","                                print(\"---------------TL_Model---------------\")\n","                                model = load_model(TL_model_path_file, compile=False)\n","                                y_hat  = model.predict(X_test)\n","                                #print(y_hat)\n","                                y_pred = (y_hat+0.5).astype(int)\n","                                #print(y_pred)\n","                                # print(\"Len of y_pred\", len(y_pred))\n","                                # print(\"Len of Y_test\", len(Y_test))\n","                                TL_results, c_matrix = evaluate(y_pred,Y_test)\n","                                # print(\"TL results: \", TL_results)\n","                                # print(\"TL c_matrix: \", c_matrix)\n","\n","                                print(\"---------------feature extract for Base Model---------------\")\n","                                tl_model_for_base= Model(inputs=model.input, outputs=model.layers[-2].output)\n","\n","                                #Now, let us use training features from neural network for Base------------------------\n","                                feature_extractor = tl_model_for_base.predict(X_train)\n","                                X_train_for_base = feature_extractor.reshape(feature_extractor.shape[0], -1) #This is our X input to Base\n","\n","                                print(\"---------------Base_Model---------------\")\n","                                Base_model_path_file = f\"{model_path}{epoch_size}/{target_id}/{Base_Model}\"\n","                                # print(\"Base_model_path_file\", Base_model_path_file)\n","\n","                                # print(os.path.isfile(Base_model_path_file))\n","                                Base_model = joblib.load(Base_model_path_file)\n","\n","                                # Train the model on training data\n","                                Base_model.fit(X_train_for_base, Y_train.ravel()) #For sklearn no one hot encoding\n","\n","                                #Send test data through same feature extractor process---------------\n","                                X_test_feature = tl_model_for_base.predict(X_test)\n","                                X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)\n","\n","                                #Now predict using the trained Base model.\n","                                y_test_prediction = Base_model.predict(X_test_feature)\n","                                TL_Base_results, c_matrix = evaluate(y_test_prediction.reshape(y_test_prediction.shape[0], -1),Y_test)\n","                                # print(\"TL_Base results: \", TL_Base_results)\n","                                # print(\"TL_Base c_matrix: \", c_matrix)\n","\n","                                kfold_evaluation_results_list.append(TL_Base_results)\n","                                kfold_confusion_matrix_list.append(c_matrix)\n","\n","                            avg_results = pd.DataFrame(kfold_evaluation_results_list, columns=EVALUATION_METRICS).mean().values\n","                            kfold_evaluation_results_list = []\n","\n","                            avg_results = avg_results.reshape(1, 15)\n","                            avg_results_df = pd.DataFrame(avg_results, columns=EVALUATION_METRICS)\n","                            avg_results_df.index = [f\"{TL_model[:-5]}_{sample_size}\"]\n","\n","                            # print(\"data Type of df_evaluation_results\", type(df_evaluation_results))\n","                            # print(\"data Type of avg_results_df\", type(avg_results_df))\n","                            df_evaluation_results = pd.concat([df_evaluation_results, avg_results_df])\n","\n","                            confusion_matrix_file = f\"{model_path}{epoch_size}/{target_id}/{Base_model_Category}/{TL_model[:-5]}_CM\"\n","                            # print(confusion_matrix_file)\n","                            save_confusion_matrix(kfold_confusion_matrix_list, confusion_matrix_file)\n","                            kfold_confusion_matrix_list = []\n","\n","                    evaluation_results_save_path = os.path.join(f\"{model_path}{epoch_size}/{target_id}/{Base_model_Category}/{target_id}_TL_Base_total_evaluation_result_{Base_model_Category}.csv\")\n","\n","                    if os.path.isfile(evaluation_results_save_path):\n","                        df_evaluation_results.to_csv(evaluation_results_save_path, mode='a', header=False)\n","                    else:\n","                        df_evaluation_results.to_csv(evaluation_results_save_path, mode='a')\n","                    df_evaluation_results = pd.DataFrame(columns=EVALUATION_METRICS)"]},{"cell_type":"markdown","metadata":{},"source":["### 5c. Parameter Setup for the Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EVALUATION_METRICS = [\"Weighted Accuracy\", \"Sensitivity/Recall\", \"Specificity\", \"Precision_class0\", \"Precision_class1\", \"Precision_avg\", \"F1_class0\", \"F1_class1\", \"F1_avg\", \"auc_roc_score\", \"False_Discovery_Rate\", \"False_Negative_Rate\", \"False_Omission_Rate\", \"False_Positive_Rate\", \"Jaccard\"]\n","tl_arch_list = [\"1_Frozen\", \"2_Frozen\", \"3_Frozen\", \"All_Frozen\", \"All_Unfrozen\"]\n","Base_model_Category_list = ['K-NN', 'LogisticRegression', 'NaiveBayes', 'SVM'] # \"DecisionTree\" - Not Working\n","epoch_sizes = [\"100_300\"]"]},{"cell_type":"markdown","metadata":{},"source":["### 6. Function Execution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-001\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-003\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-008\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-011\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-012\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-022\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-023\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-025\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-028\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-033\"\n","# TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-037\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-043\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-056\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#target_id = \"SB-059\"\n","#TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-060\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#target_id = \"SB-070\"\n","#TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-071\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-073\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-078\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-079\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-080\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-082\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-083\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-089\"\n","# TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_id = \"SB-112\"\n","TL_and_Base_Model(target_id, TL_Models, Base_Models, output_path, datadict, sample_size_dict, EVALUATION_METRICS, tl_arch_list, Base_model_Category_list, epoch_sizes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":0}
